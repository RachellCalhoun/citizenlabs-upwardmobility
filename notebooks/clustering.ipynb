{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z75CX6_Kdftb"
   },
   "outputs": [],
   "source": [
    "# !pip install geopandas\n",
    "# !pip install pygeos\n",
    "# !pip install gpdvega\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import ward, dendrogram, complete, average, single\n",
    "\n",
    "# import geopandas as gpd\n",
    "# import pygeos\n",
    "# import gpdvega \n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21350,
     "status": "ok",
     "timestamp": 1660839236346,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "5VvtD_bUdiHj",
    "outputId": "139c9d17-014b-4602-cbc8-e21e5aa7c5bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    from google.colab import  drive\n",
    "    drive.mount('/drive')\n",
    "    data_path = '/drive/Shared drives/Capstone/notebooks/data'\n",
    "else:\n",
    "    data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "executionInfo": {
     "elapsed": 1704,
     "status": "ok",
     "timestamp": 1660839238044,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "AkiuQ-pEdj0j",
    "outputId": "520f0f9a-0af2-48e1-b8af-db3e24d90ba3"
   },
   "outputs": [],
   "source": [
    "#path = f'{data_path}/processed/counties_merged_FIPS_str.csv'\n",
    "\n",
    "path = f'{data_path}/processed/counties_merged.csv'\n",
    "df = pd.read_csv(path,   dtype={\"fips\": str})\n",
    "df['FIPS'] = df['FIPS'].astype(int).astype(str).str.zfill(5)\n",
    "\n",
    "df['population'].dropna(inplace=True)\n",
    "df['HPSA Score'] = df['HPSA Score'].fillna(0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iw-ZHjfoq7j"
   },
   "outputs": [],
   "source": [
    "# df['population']\n",
    "\n",
    "# cols = ['white', 'black', 'native_american', 'asian', 'hawaiian',\n",
    "#        'some_other_race_alone', 'two_more_races', 'hispanic_or_latino',\n",
    "#        'median_family_income', 'income_20_percentile', 'income_80_percentile',\n",
    "#        'median_family_income_white', 'median_family_income_black',\n",
    "#        'median_family_income_indigenous', 'median_family_income_asian',\n",
    "#        'median_family_income_hispanic', 'preschool_enrollment_white',\n",
    "#        'preschool_enrollment_black', 'preschool_enrollment_hispanic',\n",
    "#        'preschool_enrollment_indigenous', 'preschool_enrollment_asian',\n",
    "#        'all_in_poverty', 'year', 'public_students_pre_12',\n",
    "#        'white_employed_16_64', 'black_employed_16_64',\n",
    "#        'american_indian_employed_16_64', 'asian_employed_16_64',\n",
    "#        'some_other_race_alone_employed_16_64',\n",
    "#        'two_or_more_race_employed_16_64', 'hispanic_or_latino_employed_16_64',\n",
    "#        'employed_25_54_population', 'employed_16_64_population',\n",
    "#        'preschool_enroll', 'white_under_5', 'black_under_5',\n",
    "#        'indigenous_under_5', 'asian_under_5', 'hispanic_under_5',\n",
    "#        'two_or_more_race_under_5', 'some_other_race_under_5',\n",
    "#        'avg_edu_prof_diff', 'low_birth_rate',\n",
    "#        'Not Hispanic or Latino_low_birth_rate',\n",
    "#        'Hispanic or Latino_low_birth_rate',\n",
    "#        'Unknown or Not Stated_low_birth_rate',\n",
    "#        'Black or African American_low_birth_rate', 'White_low_birth_rate',\n",
    "#        'Asian_low_birth_rate', 'More than one race_low_birth_rate',\n",
    "#        'American Indian or Alaska Native_low_birth_rate',\n",
    "#        'Native Hawaiian or Other Pacific Islander_low_birth_rate',\n",
    "#        'HPSA Score', 'HOM_STUDENTS', 'proportion_homeless', 'proportion_voter',\n",
    "#        'transit_trips_index', 'transit_low_cost_index', 'final_2019_crimes',\n",
    "#        'crime_rate', 'Total_Juvenile_Arrests', 'juvenile_crime_rate',\n",
    "#        'hispanic_or_latino_exposure', 'white_exposure', 'black_exposure',\n",
    "#        'native_american_exposure', 'asian_exposure', 'hawaiian_exposure',\n",
    "#        'some_other_race_alone_exposure', 'two_more_races_exposure',\n",
    "#        'proportion_high_poverty_neighborhood']\n",
    "#cols = ['some_other_race_alone_exposure']\n",
    "\n",
    "# create weighted average for each of the metrics\n",
    "# weighted = []\n",
    "# weighted2 = []\n",
    "# not_weighted = []\n",
    "# for col in cols:\n",
    "#   weighted.append((df['population']*df[col]/(df['population'].sum())).sum())\n",
    "#   not_weighted.append(df[col].mean())\n",
    "\n",
    "# df2 = pd.DataFrame(\n",
    "#     {'metric': cols,\n",
    "#      'weighted': weighted,\n",
    "#      'not_weighted': not_weighted\n",
    "#     })\n",
    "\n",
    "# df2.to_csv(f'{data_path}/interim/weighted_and_unweighted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1660839250543,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "NoC0bASDd_rU",
    "outputId": "9e3e1d7c-ebde-419c-d28b-2423caa89e80"
   },
   "outputs": [],
   "source": [
    "cols = ['median_family_income', 'income_20_percentile', 'income_80_percentile',\n",
    "       'median_family_income_white', 'median_family_income_black',\n",
    "       'median_family_income_hispanic', \n",
    "       'white_employed_16_64', 'black_employed_16_64',\n",
    "       'hispanic_or_latino_employed_16_64',\n",
    "       'employed_25_54_population',\n",
    "       'preschool_enroll', \n",
    "       'avg_edu_prof_diff', #'low_birth_rate', too many missing values\n",
    "       'HPSA Score', 'HOM_STUDENTS', 'proportion_homeless', 'proportion_voter',\n",
    "       'transit_trips_index', 'transit_low_cost_index',\n",
    "       'crime_rate', 'juvenile_crime_rate','debt_all', 'AQI',\n",
    "       'proportion_high_poverty_neighborhood']\n",
    "\n",
    "#cols = ['proportion_high_poverty_neighborhood', 'white_exposure', 'black_exposure', 'transit_trips_index', 'transit_low_cost_index', 'AQI','crime_rate', 'juvenile_crime_rate']\n",
    "df[cols]=df[cols].fillna(df.mean().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbbmFjqp0aQu"
   },
   "outputs": [],
   "source": [
    "county_list_mapper = list(df['FIPS'])\n",
    "\n",
    "county_dict_mapper = (df[['FIPS']].to_dict('index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1660839257589,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "LZFwO-oNerae",
    "outputId": "34b9e910-7ae4-4c91-9352-e465f150d420"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Identify and drop non-numeric fields\n",
    "#non_numeric = df.select_dtypes(exclude='number').columns.to_list()\n",
    "#us_df = df.drop(columns=non_numeric)\n",
    "\n",
    "\n",
    "#Remove fields that are numeric, but not meaningful features of the county\n",
    "us_df=df[cols]\n",
    "#us_df = us_df.drop(columns = {'state','county','year'})\n",
    "us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1660839265850,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "QK8q3POJfihd",
    "outputId": "afe5d4ea-4116-47c4-f0c8-8f47c7333bac"
   },
   "outputs": [],
   "source": [
    "print(us_df.shape)\n",
    "print(len(county_dict_mapper))\n",
    "\n",
    "{k:v for (k,v) in county_dict_mapper.items() if k < 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1660839273721,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "XgzE15CFfuZj",
    "outputId": "ae8ec858-f97f-41f2-b20a-2afe2ff443dd"
   },
   "outputs": [],
   "source": [
    "#Store the column names for later use\n",
    "us_df_columns = us_df.columns\n",
    "us_df_columns\n",
    "\n",
    "us_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikbul9R8f6ym"
   },
   "source": [
    "### Feature Scaling\n",
    "We'll use min-max normalization since many of our features are on different scales, and also numeric. May be worth changing this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1660839353646,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "GomYq5e_gCdy",
    "outputId": "6cb1acd8-a4d8-4283-bf2b-a71171fa51cb"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(us_df)\n",
    "\n",
    "# verify that there are no missing values and inspect data\n",
    "pd.DataFrame(scaled_data).isna().sum().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1660839357401,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "0TKyOm2ng_5F",
    "outputId": "f41bc09f-af04-474d-db87-b4c862525457"
   },
   "outputs": [],
   "source": [
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuE6Lr_ihTZA"
   },
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "We will run K-Means with multiple n_clusters values so we can use the elbow method. To determine the optimal number of clusters, we have to select the value of k at the “elbow”, or the point after which the inertia start decreasing in a linear fashion. \n",
    "\n",
    "Inertia is the sum of squared distances of samples to their closest cluster center (SSE)\n",
    "\n",
    "We will try two methods of initializing K-Means, the default, which is 'k-means++', which selectes inital cluster centers in a \"smart way\", and 'random' which choses initial centroids at random.\n",
    "\n",
    "We will also look at two metrics to check the quality of our clusters:\n",
    "\n",
    "#### Davies_Bouldin Index\n",
    "This index signifies the average ‘similarity’ between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score. Values closer to zero indicate a better partition.\n",
    "\n",
    "#### Calinski-Harabasz Index\n",
    "The index is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7721,
     "status": "ok",
     "timestamp": 1660839371297,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "TGbiLigThSVu",
    "outputId": "3af53db4-d771-4cfe-bc7d-a6683e084733"
   },
   "outputs": [],
   "source": [
    "k_range = [*range(3, 10)]\n",
    "\n",
    "#Calculate inertia scores using the default initialization 'k-means++'\n",
    "inertia_scores = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42,init ='k-means++').fit(scaled_data)\n",
    "    inertia_scores.append(kmeans.inertia_)\n",
    "    kmeans.fit(scaled_data)\n",
    "    ch_score = metrics.calinski_harabasz_score(scaled_data,kmeans.labels_)\n",
    "    db_score = metrics.davies_bouldin_score(scaled_data,kmeans.labels_)\n",
    "    print('k: {}, ch_score: {}, db_score {} '.format(k,ch_score,db_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1660839371510,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "vVUboR6eh5Mk",
    "outputId": "5da31ef1-bba3-413f-ec5a-e401d62e29e4"
   },
   "outputs": [],
   "source": [
    "plt.plot(k_range, inertia_scores, '-o')\n",
    "plt.ylabel('SSE')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.title('The Elbow Method using \"k-means++\" initialization ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBV0Y_aJiJp5"
   },
   "source": [
    "It looks like the elbow occurs at around 4 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NFELLrKiMTC"
   },
   "source": [
    "Let's try using silhouette analysis for 3-8 clusters.\n",
    "\n",
    "From https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py:\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 34088,
     "status": "ok",
     "timestamp": 1660839405588,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "fFekrUlriI0S",
    "outputId": "b09bfc60-5979-4ac1-8f61-a7cb08b42ef5"
   },
   "outputs": [],
   "source": [
    "# Code from https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "# Per the article, a good number of clusters will have a well above 0.5 silhouette \n",
    "# average score as well as all of the clusters have higher than the average score\n",
    "\n",
    "for i, k in enumerate(range(3,8)):\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(n_clusters=k)\n",
    "    labels = km.fit_predict(scaled_data)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(scaled_data, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    print('Average silhouette score for {} clusters is {}'.format(k,avg_score))\n",
    "    \n",
    "   # plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K74wsmyajFbA"
   },
   "source": [
    "The highest score is for 3 clusters, depending on the run .45.\n",
    "Maybe there are too many dimensions? Can try running PCA on the data, then K-Means, later. For now, lst's try another type of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30x67vMSjIm8"
   },
   "source": [
    "Agglomerative Clustering\n",
    "Agglomerative clustring performs a bottom up approach where each observation starts in its own cluster and clusters are successively merged together.\n",
    "\n",
    "There are 4 types of linkages we will consider:\n",
    "\n",
    "Single: minimum distance between clusters\n",
    "Complete: maximum distance between clusters\n",
    "Average: average distance between clusters\n",
    "Ward: difference between:\n",
    "The total within-cluster sum of squares for the two clusters seperately\n",
    "and\n",
    "The within-cluster sum of squares resulting from merging the two clusters\n",
    "We are using Ward's Method to start because it tends to create equal sized clusters and is effective for noisy data.\n",
    "\n",
    "The output of the ward function is as follows:\n",
    "\n",
    "Column 1 and 2 are child nodes\n",
    "Column 3 is distance\n",
    "Column 4 is the number of leaf nodes merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 79332,
     "status": "ok",
     "timestamp": 1660839484909,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "bSumTSPTjMym",
    "outputId": "867d3ca1-c7c3-41a9-d40a-c09785184daf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "cls1 = ward(scaled_data)\n",
    "dendrogram(cls1)#,orientation='left')\n",
    "plt.title('Dendrogram using Ward Linkage')\n",
    "plt.show()\n",
    "\n",
    "print(cls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPhm-Xf9jhcS"
   },
   "source": [
    "Based on the dendrogram we are going to assign the data to 4 clusters, so now we will run the clusters to get the assignment value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1660839485474,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "WjVqoP3mji2r",
    "outputId": "02ceb4fe-9b65-48b7-eff2-64f9936b30db"
   },
   "outputs": [],
   "source": [
    "cls_w = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "cls_assignment_w = cls_w.fit_predict(scaled_data)\n",
    "cls_assignment_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1660839485475,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "xX5AuG-EkWaW",
    "outputId": "e686efb8-d2e7-4225-ac93-18795cfb1cf3"
   },
   "outputs": [],
   "source": [
    "county_cluster = list(zip(county_list_mapper, list(cls_assignment_w)))\n",
    "\n",
    "cluster_df = pd.DataFrame(county_cluster, columns=['FIPS', 'agglom_ward_cluster'])\n",
    "(cluster_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1660839485506,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "BFXeoeubjmqs",
    "outputId": "60dbc103-6cdd-4ee5-b7da-3954486a042b"
   },
   "outputs": [],
   "source": [
    "#Zip together the mapper and the cluster assignment\n",
    "\n",
    "#Show the clusters on a map\n",
    "us_all = pd.merge(df,cluster_df,how='left',on='FIPS')\n",
    "\n",
    "# # remove Hawaii and Alaska\n",
    "# us_all = us_all[us_all['State_Name'] != 'Hawaii']\n",
    "# us_all = us_all[us_all['State_Name'] != 'Alaska']\n",
    "\n",
    "# len(us_all)\n",
    "\n",
    "us_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 22063,
     "status": "ok",
     "timestamp": 1660839507524,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "H-8SEs7PmVCB",
    "outputId": "e505e134-2139-40d6-a8b9-e0f3d22840f6"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "    \n",
    "us_all[\"agglom_ward_cluster\"] = us_all[\"agglom_ward_cluster\"].astype(str)\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.choropleth(us_all, geojson=counties, locations='FIPS', color='agglom_ward_cluster',\n",
    "                           scope=\"usa\",\n",
    "                           labels={'agglom_ward_cluster':'agglom_ward_cluster'}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eitHz7fRqM-W"
   },
   "source": [
    "Out of curiousity, let's see if we get similar clusterings if we use different agglomerative clustering linkage methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 67108,
     "status": "ok",
     "timestamp": 1660839574594,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "e0ksSOh1qN6D",
    "outputId": "fd1a278e-2bc2-46d3-afe5-0fee1e968c5b"
   },
   "outputs": [],
   "source": [
    "#Complete linkage method\n",
    "plt.figure(figsize=(10,6))\n",
    "cls2 = complete(scaled_data)\n",
    "dendrogram(cls2)#,orientation='left')\n",
    "plt.title('Dendrogram using Complete Linkage')\n",
    "plt.show()\n",
    "\n",
    "#print(cls2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANA6o4U8qdKo"
   },
   "source": [
    "With complete linkage it seems that 12? clusters are more appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKjzX6e4qdkY"
   },
   "outputs": [],
   "source": [
    "\n",
    "cls_c = AgglomerativeClustering(n_clusters=8, linkage='complete')\n",
    "cls_assignment_c = cls_c.fit_predict(scaled_data)\n",
    "\n",
    "county_cluster = list(zip(county_list_mapper, list(cls_assignment_c)))\n",
    "\n",
    "new_df = pd.DataFrame(county_cluster, columns=['FIPS', 'agglom_complete_cluster'])\n",
    "\n",
    "us_all = us_all.merge(new_df, how = 'left', on = 'FIPS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660839574981,
     "user": {
      "displayName": "Elizabeth Giancola",
      "userId": "12599460371540732383"
     },
     "user_tz": 240
    },
    "id": "lMkppgERty0u",
    "outputId": "1245d64d-21da-4524-9154-d92501c4e963"
   },
   "outputs": [],
   "source": [
    "us_all['agglom_complete_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FFDUm7p5cEp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xea0xCpErQA3",
    "outputId": "59484af6-fccd-4acc-81e4-3857e4e83408"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "us_all[\"agglom_complete_cluster\"] = us_all[\"agglom_complete_cluster\"].astype(str)\n",
    "\n",
    "\n",
    "fig = px.choropleth(us_all, geojson=counties, locations='FIPS', color='agglom_complete_cluster',\n",
    "                           scope=\"usa\",\n",
    "                            #hover_data=[\"agglom_complete_cluster\",'population', 'County_Name'],\n",
    "                           #labels={'agglom_complete_cluster':'agglom_complete_cluster','population':'Population'},\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04lwUF10rODA"
   },
   "outputs": [],
   "source": [
    "#Average Linkage Method\n",
    "plt.figure(figsize=(10,6))\n",
    "cls3 = average(scaled_data)\n",
    "dendrogram(cls3)#,orientation='left')\n",
    "plt.title('Dendrogram using Average Linkage')\n",
    "plt.show()\n",
    "\n",
    "#print(cls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mFETFcP6Pn3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeGiiKwi6Z1p"
   },
   "source": [
    "### PCA\n",
    "We are working with a lot of features in the US. Maybe this is impacting the ability to create useful clusters. Let's try PCA to help identify the most relevant features and to reduce the dimensionality so we can try clustering again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5kWLWlj6ium"
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "pca_data = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1GxB33J6oeX"
   },
   "source": [
    "Let's plot and check the variance of the components using a variance ratio plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WobgL96I6hNt"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "lbls = [str(x) for x in range(1,len(var)+1)]\n",
    "plt.bar(x=range(1,len(var)+1), height = var, tick_label = lbls)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance %')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lEt0c-x6wuC"
   },
   "outputs": [],
   "source": [
    "#Plot the magnitude of each feature value for the 2 principal components\n",
    "fig, ax = plt.subplots(figsize=(20, 18))\n",
    "plt.imshow(pca.components_[0:2].T, interpolation = 'none', cmap = 'plasma')\n",
    "feature_names=list(us_df_columns)\n",
    "plt.yticks(np.arange(-0., len(feature_names), 1) , feature_names, fontsize=10)\n",
    "plt.xticks(np.arange(0., 2, 1), ['First PC', 'Second PC'], rotation = 90, fontsize = 16)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gOmrMZI6_D_"
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca.components_.T, index=us_df_columns)\n",
    "pca_df['PC-1'] = abs(pca_df[0])\n",
    "pca_df['PC-2'] = abs(pca_df[1])\n",
    "\n",
    "pca_df['PC-1'].nlargest(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrDu5gri7GDw"
   },
   "outputs": [],
   "source": [
    "pca_df['PC-2'].nlargest(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npKU-gkp7T5a"
   },
   "source": [
    "Biplot\n",
    "The cosine of the angle between any two variable markers (vectors) is the coefficient of correlation between those variables.\n",
    "Note - this only holds if first two PC capture 80% of variance which is NOT the case here.\n",
    "\n",
    "The cosine of the angle between a vector and the axis for a given PC is the coefficient of correlation between those two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEJb2EUh7Oll"
   },
   "outputs": [],
   "source": [
    "#From SIADS 543\n",
    "feature_subset_count = 10\n",
    "\n",
    "### Feel free to use this routine to plot your own biplots!\n",
    "def biplot(score, coeff, maxdim, pcax, pcay, labels=None):\n",
    "    zoom = 0.5\n",
    "    pca1=pcax-1\n",
    "    pca2=pcay-1\n",
    "    xs = score[:,pca1]\n",
    "    ys = score[:,pca2]\n",
    "    n = min(coeff.shape[0], maxdim)\n",
    "    width = 2.0 * zoom\n",
    "    scalex = width/(xs.max()- xs.min())\n",
    "    scaley = width/(ys.max()- ys.min())\n",
    "    text_scale_factor = 1.3\n",
    "        \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    plt.scatter(xs*scalex, ys*scaley, s=9)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],\n",
    "                  color='b',alpha=0.9, head_width = 0.03 * zoom) \n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, \n",
    "                     coeff[i,pca2] * text_scale_factor, \n",
    "                     \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, \n",
    "                     coeff[i,pca2] * text_scale_factor, \n",
    "                     labels[i], color='black', ha='center', va='center')\n",
    "    \n",
    "    plt.xlim(-zoom,zoom)\n",
    "    plt.ylim(-zoom,zoom)\n",
    "    plt.xlabel(\"PC{}\".format(pcax))\n",
    "    plt.ylabel(\"PC{}\".format(pcay))\n",
    "    plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "feature_subset = slice(0, feature_subset_count, 1)\n",
    "\n",
    "biplot(pca_data, np.transpose(pca.components_[0:2, feature_subset]), \n",
    "       feature_subset_count, 1, 2, labels=feature_names[feature_subset])\n",
    "\n",
    "print(\"explained_variance_ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"sum of explained variance ratios:\", np.sum(pca.explained_variance_ratio_))\n",
    "print(\"singular values:\", pca.singular_values_)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWiJ-Esl750u"
   },
   "source": [
    "### K-Means after PCA\n",
    "\n",
    "Now we will train our model based on the new features generated by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAszYbV974E3"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=52).fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUJI1z-h70LQ"
   },
   "outputs": [],
   "source": [
    "#NOTE - for some reason, you need to run this again seperately in order for the cluster centers\n",
    "# to print in the correct spot\n",
    "centers = np.array(kmeans.cluster_centers_)\n",
    "#print(centers)\n",
    "\n",
    "pca_label = kmeans.fit_predict(pca_data)\n",
    "plt.figure(figsize=(10,10))\n",
    "uniq = np.unique(pca_label)\n",
    "for i in uniq:\n",
    "   plt.scatter(pca_data[pca_label == i , 0] , pca_data[pca_label == i , 1] , label = i)\n",
    "plt.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')\n",
    "#This is done to find the centroid for each clusters.\n",
    "plt.legend()\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "274js88P8TjC"
   },
   "outputs": [],
   "source": [
    "\n",
    "county_cluster = list(zip(county_list_mapper, list(pca_label)))\n",
    "\n",
    "new_df = pd.DataFrame(county_cluster, columns=['name', 'kmeans_pca_cluster'])\n",
    "new_df\n",
    "\n",
    "us_all = us_all.merge(new_df, how='left',left_on='FIPS',right_on='name')\n",
    "us_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z94xLad9RTS"
   },
   "outputs": [],
   "source": [
    "us_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3XeHF_O8X5U"
   },
   "outputs": [],
   "source": [
    "#Show K-Means after PCA clusters map\n",
    "\n",
    "\n",
    "fig = px.choropleth(us_all, geojson=counties, locations='FIPS', color='kmeans_pca_cluster',\n",
    "                           scope=\"usa\",\n",
    "                            #hover_data=[\"agglom_complete_cluster\",'population', 'County_Name'],\n",
    "                           #labels={'agglom_complete_cluster':'agglom_complete_cluster','population':'Population'},\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8ab7SCtAErQ"
   },
   "outputs": [],
   "source": [
    "us_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHauE3vs8j5X"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(x='kmeans_pca_cluster', y='transit_low_cost_index',\n",
    "                    data=us_all, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlVrd1NCAtN4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(x='kmeans_pca_cluster', y='transit_trips_index',\n",
    "                    data=us_all, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kjxf5l0vBMYd"
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=us_all, x=\"AQI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clustering Liz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
